
=================== Fold 1 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 1 with MAE: 129.22020362405215

Training Loss: 17753.51, MAE: 128.24
Validation Loss: 17877.18, MAE: 129.22
Epoch 1/3 took: 0:07:25

========== Epoch 2 / 3 ==========
New best model found at fold 1 with MAE: 129.16034025304458

Training Loss: 17679.73, MAE: 127.97
Validation Loss: 17861.48, MAE: 129.16
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
New best model found at fold 1 with MAE: 129.13846700331743

Training Loss: 17679.16, MAE: 127.96
Validation Loss: 17855.76, MAE: 129.14
Epoch 3/3 took: 0:07:24
================================

Fold 1 Average Metrics

fold_1_train_loss_values: [17753.50662037521, 17679.726696409907, 17679.15994068771]
fold_1_val_loss_values: [17877.182057100184, 17861.482996323528, 17855.759191176472]
fold_1_train_mae_values: [128.23603172684992, 127.96715895228562, 127.96110292262456]
fold_1_val_mae_values: [129.22020362405215, 129.16034025304458, 129.13846700331743]
Average Training Loss: 17704.13
Average Training MAE: 128.05
Average Validation Loss: 17864.81
Average Validation MAE: 129.17

Fold 1 took: 0:22:14

=================== Fold 2 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 2 with MAE: 128.32982702816233

Training Loss: 17678.74, MAE: 128.00
Validation Loss: 17883.64, MAE: 128.33
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17671.98, MAE: 127.97
Validation Loss: 17883.64, MAE: 128.33
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17674.45, MAE: 127.97
Validation Loss: 17883.64, MAE: 128.33
Epoch 3/3 took: 0:07:23
================================

Fold 2 Average Metrics

fold_2_train_loss_values: [17678.740427074623, 17671.977297371446, 17674.45105429557]
fold_2_val_loss_values: [17883.63556985294, 17883.63556985294, 17883.63556985294]
fold_2_train_mae_values: [127.99649963889233, 127.9702391608503, 127.97438745913298]
fold_2_val_mae_values: [128.32982702816233, 128.32982702816233, 128.32982702816233]
Average Training Loss: 17675.06
Average Training MAE: 127.98
Average Validation Loss: 17883.64
Average Validation MAE: 128.33

Fold 2 took: 0:22:12

=================== Fold 3 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 3 with MAE: 127.66759670481963

Training Loss: 17698.96, MAE: 128.03
Validation Loss: 17451.31, MAE: 127.67
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17715.02, MAE: 128.09
Validation Loss: 17451.31, MAE: 127.67
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17718.56, MAE: 128.10
Validation Loss: 17451.31, MAE: 127.67
Epoch 3/3 took: 0:07:24
================================

Fold 3 Average Metrics

fold_3_train_loss_values: [17698.96097342705, 17715.02354201505, 17718.557208925584]
fold_3_val_loss_values: [17451.309627757353, 17451.309627757353, 17451.309627757353]
fold_3_train_mae_values: [128.02852755645446, 128.0924509871365, 128.10223475427532]
fold_3_val_mae_values: [127.66759670481963, 127.66759670481963, 127.66759670481963]
Average Training Loss: 17710.85
Average Training MAE: 128.07
Average Validation Loss: 17451.31
Average Validation MAE: 127.67

Fold 3 took: 0:22:12

=================== Fold 4 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 4 with MAE: 127.53904140696807

Training Loss: 17710.70, MAE: 128.14
Validation Loss: 17552.05, MAE: 127.54
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17714.78, MAE: 128.16
Validation Loss: 17552.05, MAE: 127.54
Epoch 2/3 took: 0:07:25

========== Epoch 3 / 3 ==========
Training Loss: 17711.97, MAE: 128.15
Validation Loss: 17552.05, MAE: 127.54
Epoch 3/3 took: 0:07:24
================================

Fold 4 Average Metrics

fold_4_train_loss_values: [17710.701488686245, 17714.782043661162, 17711.972597460284]
fold_4_val_loss_values: [17552.04855526195, 17552.04855526195, 17552.04855526195]
fold_4_train_mae_values: [128.14228823352417, 128.15953954486145, 128.1514408022265]
fold_4_val_mae_values: [127.53904140696807, 127.53904140696807, 127.53904140696807]
Average Training Loss: 17712.49
Average Training MAE: 128.15
Average Validation Loss: 17552.05
Average Validation MAE: 127.54

Fold 4 took: 0:22:13

=================== Fold 5 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 5 with MAE: 125.22316001443302

Training Loss: 17773.10, MAE: 128.39
Validation Loss: 16963.90, MAE: 125.22
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17782.73, MAE: 128.42
Validation Loss: 16963.90, MAE: 125.22
Epoch 2/3 took: 0:07:25

========== Epoch 3 / 3 ==========
Training Loss: 17763.09, MAE: 128.35
Validation Loss: 16963.90, MAE: 125.22
Epoch 3/3 took: 0:07:24
================================

Fold 5 Average Metrics

fold_5_train_loss_values: [17773.100148280726, 17782.7297012176, 17763.08841320025]
fold_5_val_loss_values: [16963.901783662684, 16963.901783662684, 16963.901783662684]
fold_5_train_mae_values: [128.3881196497276, 128.42309978574414, 128.34842329758865]
fold_5_val_mae_values: [125.22316001443302, 125.22316001443302, 125.22316001443302]
Average Training Loss: 17772.97
Average Training MAE: 128.39
Average Validation Loss: 16963.90
Average Validation MAE: 125.22

Fold 5 took: 0:22:13

=================== Fold 6 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17712.37, MAE: 128.15
Validation Loss: 17420.46, MAE: 126.80
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17688.11, MAE: 128.03
Validation Loss: 17420.46, MAE: 126.80
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17686.34, MAE: 128.02
Validation Loss: 17420.46, MAE: 126.80
Epoch 3/3 took: 0:07:24
================================

Fold 6 Average Metrics

fold_6_train_loss_values: [17712.3746505278, 17688.110407086122, 17686.343864313338]
fold_6_val_loss_values: [17420.461583754597, 17420.461583754597, 17420.461583754597]
fold_6_train_mae_values: [128.14744029475693, 128.02508190244336, 128.02129665106833]
fold_6_val_mae_values: [126.8037769093233, 126.8037769093233, 126.8037769093233]
Average Training Loss: 17695.61
Average Training MAE: 128.06
Average Validation Loss: 17420.46
Average Validation MAE: 126.80

Fold 6 took: 0:22:13

=================== Fold 7 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 7 with MAE: 124.45087589937098

Training Loss: 17768.16, MAE: 128.43
Validation Loss: 16985.25, MAE: 124.45
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17756.09, MAE: 128.39
Validation Loss: 16985.25, MAE: 124.45
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17754.07, MAE: 128.39
Validation Loss: 16985.25, MAE: 124.45
Epoch 3/3 took: 0:07:24
================================

Fold 7 Average Metrics

fold_7_train_loss_values: [17768.16136143917, 17756.091473531564, 17754.065720369985]
fold_7_val_loss_values: [16985.25110581342, 16985.25110581342, 16985.25110581342]
fold_7_train_mae_values: [128.43147318658222, 128.39418739140234, 128.38652061538951]
fold_7_val_mae_values: [124.45087589937098, 124.45087589937098, 124.45087589937098]
Average Training Loss: 17759.44
Average Training MAE: 128.40
Average Validation Loss: 16985.25
Average Validation MAE: 124.45

Fold 7 took: 0:22:12

=================== Fold 8 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17794.31, MAE: 128.51
Validation Loss: 16884.51, MAE: 124.60
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17783.61, MAE: 128.46
Validation Loss: 16884.51, MAE: 124.60
Epoch 2/3 took: 0:07:23

========== Epoch 3 / 3 ==========
Training Loss: 17775.37, MAE: 128.41
Validation Loss: 16884.51, MAE: 124.60
Epoch 3/3 took: 0:07:24
================================

Fold 8 Average Metrics

fold_8_train_loss_values: [17794.31365619774, 17783.612755408652, 17775.373624973872]
fold_8_val_loss_values: [16884.5146484375, 16884.5146484375, 16884.5146484375]
fold_8_train_mae_values: [128.5081871313395, 128.46441425846572, 128.4081896778732]
fold_8_val_mae_values: [124.59793786441578, 124.59793786441578, 124.59793786441578]
Average Training Loss: 17784.43
Average Training MAE: 128.46
Average Validation Loss: 16884.51
Average Validation MAE: 124.60

Fold 8 took: 0:22:11

=================== Fold 9 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17664.61, MAE: 127.95
Validation Loss: 18007.13, MAE: 129.45
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17657.06, MAE: 127.92
Validation Loss: 18007.13, MAE: 129.45
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17646.88, MAE: 127.87
Validation Loss: 18007.13, MAE: 129.45
Epoch 3/3 took: 0:07:24
================================

Fold 9 Average Metrics

fold_9_train_loss_values: [17664.611128893186, 17657.061484244357, 17646.88089530205]
fold_9_val_loss_values: [18007.132331399356, 18007.132331399356, 18007.132331399356]
fold_9_train_mae_values: [127.94779307387745, 127.91886401734624, 127.87402552984231]
fold_9_val_mae_values: [129.4508086933809, 129.4508086933809, 129.4508086933809]
Average Training Loss: 17656.18
Average Training MAE: 127.91
Average Validation Loss: 18007.13
Average Validation MAE: 129.45

Fold 9 took: 0:22:12

=================== Fold 10 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17622.59, MAE: 127.74
Validation Loss: 18047.57, MAE: 129.58
Epoch 1/3 took: 0:07:24

========== Epoch 2 / 3 ==========
Training Loss: 17626.06, MAE: 127.77
Validation Loss: 18047.57, MAE: 129.58
Epoch 2/3 took: 0:07:24

========== Epoch 3 / 3 ==========
Training Loss: 17640.23, MAE: 127.82
Validation Loss: 18047.57, MAE: 129.58
Epoch 3/3 took: 0:07:24
================================

Fold 10 Average Metrics

fold_10_train_loss_values: [17622.5945175324, 17626.064929974917, 17640.231030518396]
fold_10_val_loss_values: [18047.568244485294, 18047.568244485294, 18047.568244485294]
fold_10_train_mae_values: [127.74004746759216, 127.77404501924546, 127.82197216123242]
fold_10_val_mae_values: [129.5793654497932, 129.5793654497932, 129.5793654497932]
Average Training Loss: 17629.63
Average Training MAE: 127.78
Average Validation Loss: 18047.57
Average Validation MAE: 129.58

Fold 10 took: 0:22:13
overall_train_loss_values[17753.50662037521, 17679.726696409907, 17679.15994068771, 17678.740427074623, 17671.977297371446, 17674.45105429557, 17698.96097342705, 17715.02354201505, 17718.557208925584, 17710.701488686245, 17714.782043661162, 17711.972597460284, 17773.100148280726, 17782.7297012176, 17763.08841320025, 17712.3746505278, 17688.110407086122, 17686.343864313338, 17768.16136143917, 17756.091473531564, 17754.065720369985, 17794.31365619774, 17783.612755408652, 17775.373624973872, 17664.611128893186, 17657.061484244357, 17646.88089530205, 17622.5945175324, 17626.064929974917, 17640.231030518396]
overall_val_loss_values[17877.182057100184, 17861.482996323528, 17855.759191176472, 17883.63556985294, 17883.63556985294, 17883.63556985294, 17451.309627757353, 17451.309627757353, 17451.309627757353, 17552.04855526195, 17552.04855526195, 17552.04855526195, 16963.901783662684, 16963.901783662684, 16963.901783662684, 17420.461583754597, 17420.461583754597, 17420.461583754597, 16985.25110581342, 16985.25110581342, 16985.25110581342, 16884.5146484375, 16884.5146484375, 16884.5146484375, 18007.132331399356, 18007.132331399356, 18007.132331399356, 18047.568244485294, 18047.568244485294, 18047.568244485294]
overall_train_mae_values[128.23603172684992, 127.96715895228562, 127.96110292262456, 127.99649963889233, 127.9702391608503, 127.97438745913298, 128.02852755645446, 128.0924509871365, 128.10223475427532, 128.14228823352417, 128.15953954486145, 128.1514408022265, 128.3881196497276, 128.42309978574414, 128.34842329758865, 128.14744029475693, 128.02508190244336, 128.02129665106833, 128.43147318658222, 128.39418739140234, 128.38652061538951, 128.5081871313395, 128.46441425846572, 128.4081896778732, 127.94779307387745, 127.91886401734624, 127.87402552984231, 127.74004746759216, 127.77404501924546, 127.82197216123242]
overall_val_mae_values[129.22020362405215, 129.16034025304458, 129.13846700331743, 128.32982702816233, 128.32982702816233, 128.32982702816233, 127.66759670481963, 127.66759670481963, 127.66759670481963, 127.53904140696807, 127.53904140696807, 127.53904140696807, 125.22316001443302, 125.22316001443302, 125.22316001443302, 126.8037769093233, 126.8037769093233, 126.8037769093233, 124.45087589937098, 124.45087589937098, 124.45087589937098, 124.59793786441578, 124.59793786441578, 124.59793786441578, 129.4508086933809, 129.4508086933809, 129.4508086933809, 129.5793654497932, 129.5793654497932, 129.5793654497932]

================================ Training complete! ================================
Overall summary:
Average Training Loss: 17710.08
Average Validation Loss: 17506.06
Average Training MAE: 128.13
Average Validation MAE: 127.28
All folds took: 3:42:05
Best model was from fold with MAE: 124.45
