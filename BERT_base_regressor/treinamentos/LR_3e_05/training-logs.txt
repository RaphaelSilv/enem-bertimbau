
=================== Fold 1 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 1 with MAE: 129.43868076100068

Training Loss: 17805.56, MAE: 128.44
Validation Loss: 17934.52, MAE: 129.44
Epoch 1/3 took: 0:07:53

========== Epoch 2 / 3 ==========
New best model found at fold 1 with MAE: 129.40751872343176

Training Loss: 17747.58, MAE: 128.22
Validation Loss: 17926.33, MAE: 129.41
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
New best model found at fold 1 with MAE: 129.39501818488625

Training Loss: 17756.69, MAE: 128.25
Validation Loss: 17923.06, MAE: 129.40
Epoch 3/3 took: 0:07:50
================================

Fold 1 Average Metrics

fold_1_train_loss_values: [17805.564580502716, 17747.57906563545, 17756.687238712373]
fold_1_val_loss_values: [17934.519416360294, 17926.332419002756, 17923.057818244484]
fold_1_train_mae_values: [128.44323817224407, 128.22456308830542, 128.2547148892712]
fold_1_val_mae_values: [129.43868076100068, 129.40751872343176, 129.39501818488625]
Average Training Loss: 17769.94
Average Training MAE: 128.31
Average Validation Loss: 17927.97
Average Validation MAE: 129.41

Fold 1 took: 0:23:36

=================== Fold 2 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 2 with MAE: 128.57795849968406

Training Loss: 17738.26, MAE: 128.22
Validation Loss: 17950.27, MAE: 128.58
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17759.58, MAE: 128.30
Validation Loss: 17950.27, MAE: 128.58
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17761.34, MAE: 128.30
Validation Loss: 17950.27, MAE: 128.58
Epoch 3/3 took: 0:07:51
================================

Fold 2 Average Metrics

fold_2_train_loss_values: [17738.259069946696, 17759.583275632314, 17761.34169889214]
fold_2_val_loss_values: [17950.26841107537, 17950.26841107537, 17950.26841107537]
fold_2_train_mae_values: [128.2162212001839, 128.30353416009092, 128.30263725969704]
fold_2_val_mae_values: [128.57795849968406, 128.57795849968406, 128.57795849968406]
Average Training Loss: 17753.06
Average Training MAE: 128.27
Average Validation Loss: 17950.27
Average Validation MAE: 128.58

Fold 2 took: 0:23:34

=================== Fold 3 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 3 with MAE: 127.92338113223805

Training Loss: 17777.44, MAE: 128.33
Validation Loss: 17517.64, MAE: 127.92
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17796.77, MAE: 128.39
Validation Loss: 17517.64, MAE: 127.92
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17773.24, MAE: 128.31
Validation Loss: 17517.64, MAE: 127.92
Epoch 3/3 took: 0:07:52
================================

Fold 3 Average Metrics

fold_3_train_loss_values: [17777.43745100857, 17796.76547802571, 17773.242226693143]
fold_3_val_loss_values: [17517.635627297794, 17517.635627297794, 17517.635627297794]
fold_3_train_mae_values: [128.3319879997534, 128.39346959040716, 128.3073224734303]
fold_3_val_mae_values: [127.92338113223805, 127.92338113223805, 127.92338113223805]
Average Training Loss: 17782.48
Average Training MAE: 128.34
Average Validation Loss: 17517.64
Average Validation MAE: 127.92

Fold 3 took: 0:23:33

=================== Fold 4 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 4 with MAE: 127.79283725514131

Training Loss: 17775.67, MAE: 128.39
Validation Loss: 17618.28, MAE: 127.79
Epoch 1/3 took: 0:07:50

========== Epoch 2 / 3 ==========
Training Loss: 17783.20, MAE: 128.42
Validation Loss: 17618.28, MAE: 127.79
Epoch 2/3 took: 0:07:50

========== Epoch 3 / 3 ==========
Training Loss: 17772.59, MAE: 128.37
Validation Loss: 17618.28, MAE: 127.79
Epoch 3/3 took: 0:07:51
================================

Fold 4 Average Metrics

fold_4_train_loss_values: [17775.66634876672, 17783.195434978574, 17772.594700433736]
fold_4_val_loss_values: [17618.2795122932, 17618.2795122932, 17618.2795122932]
fold_4_train_mae_values: [128.38999806279722, 128.41935492678232, 128.3743485160496]
fold_4_val_mae_values: [127.79283725514131, 127.79283725514131, 127.79283725514131]
Average Training Loss: 17777.15
Average Training MAE: 128.39
Average Validation Loss: 17618.28
Average Validation MAE: 127.79

Fold 4 took: 0:23:32

=================== Fold 5 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 5 with MAE: 125.47308136435116

Training Loss: 17838.87, MAE: 128.63
Validation Loss: 17028.90, MAE: 125.47
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17860.74, MAE: 128.72
Validation Loss: 17028.90, MAE: 125.47
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17854.27, MAE: 128.70
Validation Loss: 17028.90, MAE: 125.47
Epoch 3/3 took: 0:07:51
================================

Fold 5 Average Metrics

fold_5_train_loss_values: [17838.87134523934, 17860.741903349706, 17854.267761026338]
fold_5_val_loss_values: [17028.90172621783, 17028.90172621783, 17028.90172621783]
fold_5_train_mae_values: [128.63229673761987, 128.72459475412018, 128.69842409369937]
fold_5_val_mae_values: [125.47308136435116, 125.47308136435116, 125.47308136435116]
Average Training Loss: 17851.29
Average Training MAE: 128.69
Average Validation Loss: 17028.90
Average Validation MAE: 125.47

Fold 5 took: 0:23:33

=================== Fold 6 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17771.08, MAE: 128.36
Validation Loss: 17486.33, MAE: 127.06
Epoch 1/3 took: 0:07:52

========== Epoch 2 / 3 ==========
Training Loss: 17766.78, MAE: 128.35
Validation Loss: 17486.33, MAE: 127.06
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17770.18, MAE: 128.37
Validation Loss: 17486.33, MAE: 127.06
Epoch 3/3 took: 0:07:52
================================

Fold 6 Average Metrics

fold_6_train_loss_values: [17771.08319724603, 17766.775498406147, 17770.183347159804]
fold_6_val_loss_values: [17486.331629136028, 17486.331629136028, 17486.331629136028]
fold_6_train_mae_values: [128.35711988876497, 128.34864529000478, 128.36621343810424]
fold_6_val_mae_values: [127.05762526568245, 127.05762526568245, 127.05762526568245]
Average Training Loss: 17769.35
Average Training MAE: 128.36
Average Validation Loss: 17486.33
Average Validation MAE: 127.06

Fold 6 took: 0:23:34

=================== Fold 7 / 10 ======================

========== Epoch 1 / 3 ==========
New best model found at fold 7 with MAE: 124.7047036114861

Training Loss: 17823.52, MAE: 128.65
Validation Loss: 17049.90, MAE: 124.70
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17827.22, MAE: 128.66
Validation Loss: 17049.90, MAE: 124.70
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17823.38, MAE: 128.65
Validation Loss: 17049.90, MAE: 124.70
Epoch 3/3 took: 0:07:50
================================

Fold 7 Average Metrics

fold_7_train_loss_values: [17823.51870819398, 17827.215686402593, 17823.382593671613]
fold_7_val_loss_values: [17049.896498736212, 17049.896498736212, 17049.896498736212]
fold_7_train_mae_values: [128.65386692417107, 128.6591768296666, 128.65140814127332]
fold_7_val_mae_values: [124.7047036114861, 124.7047036114861, 124.7047036114861]
Average Training Loss: 17824.71
Average Training MAE: 128.65
Average Validation Loss: 17049.90
Average Validation MAE: 124.70

Fold 7 took: 0:23:32

=================== Fold 8 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17851.42, MAE: 128.72
Validation Loss: 16949.22, MAE: 124.85
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17866.29, MAE: 128.77
Validation Loss: 16949.22, MAE: 124.85
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17859.87, MAE: 128.75
Validation Loss: 16949.22, MAE: 124.85
Epoch 3/3 took: 0:07:51
================================

Fold 8 Average Metrics

fold_8_train_loss_values: [17851.41571514423, 17866.291900736833, 17859.86783745297]
fold_8_val_loss_values: [16949.223920036766, 16949.223920036766, 16949.223920036766]
fold_8_train_mae_values: [128.71871394536964, 128.7718250950842, 128.75419185472572]
fold_8_val_mae_values: [124.85174627865062, 124.85174627865062, 124.85174627865062]
Average Training Loss: 17859.19
Average Training MAE: 128.75
Average Validation Loss: 16949.22
Average Validation MAE: 124.85

Fold 8 took: 0:23:32

=================== Fold 9 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17731.53, MAE: 128.20
Validation Loss: 18074.37, MAE: 129.70
Epoch 1/3 took: 0:07:50

========== Epoch 2 / 3 ==========
Training Loss: 17741.17, MAE: 128.23
Validation Loss: 18074.37, MAE: 129.70
Epoch 2/3 took: 0:07:50

========== Epoch 3 / 3 ==========
Training Loss: 17736.86, MAE: 128.22
Validation Loss: 18074.37, MAE: 129.70
Epoch 3/3 took: 0:07:51
================================

Fold 9 Average Metrics

fold_9_train_loss_values: [17731.530779682274, 17741.16888325669, 17736.85697441994]
fold_9_val_loss_values: [18074.37436810662, 18074.37436810662, 18074.37436810662]
fold_9_train_mae_values: [128.20118445457024, 128.2335889171996, 128.22438927079523]
fold_9_val_mae_values: [129.70467118655935, 129.70467118655935, 129.70467118655935]
Average Training Loss: 17736.52
Average Training MAE: 128.22
Average Validation Loss: 18074.37
Average Validation MAE: 129.70

Fold 9 took: 0:23:32

=================== Fold 10 / 10 ======================

========== Epoch 1 / 3 ==========
Training Loss: 17702.68, MAE: 128.07
Validation Loss: 18114.88, MAE: 129.84
Epoch 1/3 took: 0:07:51

========== Epoch 2 / 3 ==========
Training Loss: 17707.48, MAE: 128.08
Validation Loss: 18114.88, MAE: 129.84
Epoch 2/3 took: 0:07:51

========== Epoch 3 / 3 ==========
Training Loss: 17716.97, MAE: 128.12
Validation Loss: 18114.88, MAE: 129.84
Epoch 3/3 took: 0:07:52
================================

Fold 10 Average Metrics

fold_10_train_loss_values: [17702.6783614653, 17707.48370545046, 17716.965670072117]
fold_10_val_loss_values: [18114.882783777575, 18114.882783777575, 18114.882783777575]
fold_10_train_mae_values: [128.06555193642708, 128.08180127096017, 128.11926006712642]
fold_10_val_mae_values: [129.83511442296646, 129.83511442296646, 129.83511442296646]
Average Training Loss: 17709.04
Average Training MAE: 128.09
Average Validation Loss: 18114.88
Average Validation MAE: 129.84

Fold 10 took: 0:23:33
overall_train_loss_values[17805.564580502716, 17747.57906563545, 17756.687238712373, 17738.259069946696, 17759.583275632314, 17761.34169889214, 17777.43745100857, 17796.76547802571, 17773.242226693143, 17775.66634876672, 17783.195434978574, 17772.594700433736, 17838.87134523934, 17860.741903349706, 17854.267761026338, 17771.08319724603, 17766.775498406147, 17770.183347159804, 17823.51870819398, 17827.215686402593, 17823.382593671613, 17851.41571514423, 17866.291900736833, 17859.86783745297, 17731.530779682274, 17741.16888325669, 17736.85697441994, 17702.6783614653, 17707.48370545046, 17716.965670072117]
overall_val_loss_values[17934.519416360294, 17926.332419002756, 17923.057818244484, 17950.26841107537, 17950.26841107537, 17950.26841107537, 17517.635627297794, 17517.635627297794, 17517.635627297794, 17618.2795122932, 17618.2795122932, 17618.2795122932, 17028.90172621783, 17028.90172621783, 17028.90172621783, 17486.331629136028, 17486.331629136028, 17486.331629136028, 17049.896498736212, 17049.896498736212, 17049.896498736212, 16949.223920036766, 16949.223920036766, 16949.223920036766, 18074.37436810662, 18074.37436810662, 18074.37436810662, 18114.882783777575, 18114.882783777575, 18114.882783777575]
overall_train_mae_values[128.44323817224407, 128.22456308830542, 128.2547148892712, 128.2162212001839, 128.30353416009092, 128.30263725969704, 128.3319879997534, 128.39346959040716, 128.3073224734303, 128.38999806279722, 128.41935492678232, 128.3743485160496, 128.63229673761987, 128.72459475412018, 128.69842409369937, 128.35711988876497, 128.34864529000478, 128.36621343810424, 128.65386692417107, 128.6591768296666, 128.65140814127332, 128.71871394536964, 128.7718250950842, 128.75419185472572, 128.20118445457024, 128.2335889171996, 128.22438927079523, 128.06555193642708, 128.08180127096017, 128.11926006712642]
overall_val_mae_values[129.43868076100068, 129.40751872343176, 129.39501818488625, 128.57795849968406, 128.57795849968406, 128.57795849968406, 127.92338113223805, 127.92338113223805, 127.92338113223805, 127.79283725514131, 127.79283725514131, 127.79283725514131, 125.47308136435116, 125.47308136435116, 125.47308136435116, 127.05762526568245, 127.05762526568245, 127.05762526568245, 124.7047036114861, 124.7047036114861, 124.7047036114861, 124.85174627865062, 124.85174627865062, 124.85174627865062, 129.70467118655935, 129.70467118655935, 129.70467118655935, 129.83511442296646, 129.83511442296646, 129.83511442296646]

================================ Training complete! ================================
Overall summary:
Average Training Loss: 17783.27
Average Validation Loss: 17571.78
Average Training MAE: 128.41
Average Validation MAE: 127.53
All folds took: 3:55:31
Best model was from fold with MAE: 124.70
